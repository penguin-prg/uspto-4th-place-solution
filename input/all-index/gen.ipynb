{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PACKAGE_DIR = \"/kaggle/src\"\n",
    "sys.path.append(PACKAGE_DIR)\n",
    "sys.path.append(os.path.join(PACKAGE_DIR, \"Penguin-ML-Library\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/whoosh-wheel-2-7-4/Whoosh-2.7.4-py2.py3-none-any.whl\n",
      "Whoosh is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "import whoosh\n",
    "\n",
    "import whoosh_utils\n",
    "\n",
    "# TODO: get_analyzer() をutilsに移動\n",
    "BRS_STOPWORDS = [\n",
    "    \"an\",\n",
    "    \"are\",\n",
    "    \"by\",\n",
    "    \"for\",\n",
    "    \"if\",\n",
    "    \"into\",\n",
    "    \"is\",\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"of\",\n",
    "    \"on\",\n",
    "    \"such\",\n",
    "    \"that\",\n",
    "    \"the\",\n",
    "    \"their\",\n",
    "    \"then\",\n",
    "    \"there\",\n",
    "    \"these\",\n",
    "    \"they\",\n",
    "    \"this\",\n",
    "    \"to\",\n",
    "    \"was\",\n",
    "    \"will\",\n",
    "]\n",
    "\n",
    "\n",
    "NUMBER_REGEX = re.compile(r\"^(\\d+|\\d{1,3}(,\\d{3})*)(\\.\\d+)?$\")\n",
    "\n",
    "\n",
    "class NumberFilter(whoosh.analysis.Filter):\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if not NUMBER_REGEX.match(t.text):\n",
    "                yield t\n",
    "\n",
    "\n",
    "def get_token_list(text: str) -> List[str]:\n",
    "    tokens = custom_analyzer(text)\n",
    "    return [t.text for t in tokens]\n",
    "\n",
    "\n",
    "# Prevent both stopwords and numbers from ever being indexed.\n",
    "custom_analyzer = whoosh.analysis.StandardAnalyzer(stoplist=BRS_STOPWORDS) | NumberFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>publication_number</th><th>title</th><th>abstract</th><th>claims</th><th>description</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;US-X1-I1&quot;</td><td>&quot;The making of …</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;XQQQQ Si @s EQ…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌────────────────────┬───────────────────────────────────┬──────────┬────────┬─────────────────┐\n",
       "│ publication_number ┆ title                             ┆ abstract ┆ claims ┆ description     │\n",
       "│ ---                ┆ ---                               ┆ ---      ┆ ---    ┆ ---             │\n",
       "│ str                ┆ str                               ┆ str      ┆ str    ┆ str             │\n",
       "╞════════════════════╪═══════════════════════════════════╪══════════╪════════╪═════════════════╡\n",
       "│ US-X1-I1           ┆ The making of Pot ash and Pearl … ┆          ┆        ┆ XQQQQ Si @s EQ@ │\n",
       "└────────────────────┴───────────────────────────────────┴──────────┴────────┴─────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"/kaggle/input/uspto-boolean-search-optimization/patent_data/1790_7.parquet\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "\n",
    "def save_list_bz2(data, filename):\n",
    "    serialized_data = json.dumps(data)\n",
    "    compressed_data = bz2.compress(serialized_data.encode(\"utf-8\"))\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(compressed_data)\n",
    "\n",
    "\n",
    "def load_list_bz2(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "        decompressed_data = bz2.decompress(compressed_data)\n",
    "        return json.loads(decompressed_data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2251/2251 [28:40:29<00:00, 45.86s/it]    \n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "os.makedirs(\"patent2data\", exist_ok=True)\n",
    "\n",
    "files = sorted(glob(\"/kaggle/input/uspto-boolean-search-optimization/patent_data/*.parquet\"))\n",
    "\n",
    "patent2json = {}\n",
    "patent2data = {}\n",
    "save_file_id = 0\n",
    "N_PATENT_PER_FILE = 10\n",
    "for f in tqdm(files):\n",
    "    df = pl.read_parquet(f)\n",
    "    for patent, title, abstact, claims, description in zip(\n",
    "        df[\"publication_number\"],\n",
    "        df[\"title\"],\n",
    "        df[\"abstract\"],\n",
    "        df[\"claims\"],\n",
    "        df[\"description\"],\n",
    "    ):\n",
    "        patent2json[patent] = save_file_id\n",
    "        patent2data[patent] = {\n",
    "            \"title\": get_token_list(title),\n",
    "            \"abstract\": get_token_list(abstact),\n",
    "            \"claims\": get_token_list(claims),\n",
    "            \"description\": get_token_list(description),\n",
    "        }\n",
    "\n",
    "        if len(patent2data) == N_PATENT_PER_FILE:\n",
    "            save_list_bz2(patent2data, f\"patent2data/{save_file_id}.json.bz2\")\n",
    "            patent2data = {}\n",
    "            save_file_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "save_list_bz2(patent2data, f\"patent2data/{save_file_id}.json.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"patent2json.json\", \"w\") as f:\n",
    "    json.dump(patent2json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
