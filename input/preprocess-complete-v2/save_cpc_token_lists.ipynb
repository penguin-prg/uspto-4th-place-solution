{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PACKAGE_DIR = \"/kaggle/src\"\n",
    "sys.path.append(PACKAGE_DIR)\n",
    "sys.path.append(os.path.join(PACKAGE_DIR, \"Penguin-ML-Library\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/whoosh-wheel-2-7-4/Whoosh-2.7.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.10/site-packages (from Whoosh==2.7.4) (1.5.2)\n",
      "Whoosh is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m2024-06-22 04:49:41.393754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 04:49:41.639442: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 04:49:42.732985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2024-06-22 04:49:42.733485: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2024-06-22 04:49:42.733491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "set seed: 46\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import gc\n",
    "import json\n",
    "import multiprocessing\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plyvel\n",
    "import polars as pl\n",
    "import yaml\n",
    "from penguinml.utils.logger import get_logger, init_logger\n",
    "from penguinml.utils.set_seed import seed_base\n",
    "from penguinml.utils.timer import Timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import whoosh_utils\n",
    "from const import ALL_KEYS, INF, KEY2QUERY, NUM_CPU\n",
    "from db import CpcToken2RangeDB, SingleTokenDB, TokinezedDB\n",
    "from solver import HitBlock, SimulatedAnnealing, State\n",
    "from utils import compute_ap, evaluate, load_list_bz2, save_list_bz2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "MODEL_NAME = \"baseline\"\n",
    "CFG = yaml.safe_load(open(os.path.join(PACKAGE_DIR, \"config.yaml\"), \"r\"))\n",
    "\n",
    "init_logger(\"log.log\")\n",
    "logger = get_logger(\"main\")\n",
    "seed_base(CFG[MODEL_NAME][\"execution\"][\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13307647it [23:28, 9451.27it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(\"/kaggle/input/uspto-boolean-search-optimization/nearest_neighbors.csv\")\n",
    "pub2id = {pub: idx for idx, pub in enumerate(df[\"publication_number\"].to_numpy())}\n",
    "id2pub = {idx: pub for idx, pub in enumerate(df[\"publication_number\"].to_numpy())}\n",
    "\n",
    "patent2centers = {}\n",
    "for targets in tqdm(df.iter_rows()):\n",
    "    center = targets[0]\n",
    "    center_id = pub2id[center]\n",
    "\n",
    "    targets = targets[1:]\n",
    "    assert len(targets) == 50\n",
    "    for idx, target in enumerate(targets):\n",
    "        if target not in pub2id:\n",
    "            continue\n",
    "        target_id = pub2id[target]\n",
    "        if target_id not in patent2centers:\n",
    "            patent2centers[target_id] = []\n",
    "        patent2centers[target_id].append(center_id)\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/kaggle/input/cpc-mapping/cpc2patents.json\", \"r\") as f:\n",
    "    cpc2patents = json.load(f)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r data\n",
    "# !mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265889/265889 [06:29<00:00, 682.41it/s] \n"
     ]
    }
   ],
   "source": [
    "def process_cpc_patents(args):\n",
    "    cpc, patents = args\n",
    "    n = len(patents)\n",
    "    if not (1000 <= n and n <= 20000):\n",
    "        return\n",
    "\n",
    "    cpc_save = cpc.replace(\"/\", \"-\")\n",
    "    output_path = f\"data/{cpc_save}.bz2\"\n",
    "    if os.path.exists(output_path):\n",
    "        return\n",
    "\n",
    "    # patentのテキストデータを読み込む\n",
    "    patent2tokens = {}\n",
    "    for patent in patents:\n",
    "        try:\n",
    "            path = os.path.join(\"/kaggle/input/all-index-per-patent/data\", patent + \".json.bz2\")\n",
    "            if os.path.exists(path):\n",
    "                data = load_list_bz2(path)\n",
    "            else:\n",
    "                path = os.path.join(\n",
    "                    \"/kaggle/input/all-index-per-patent/data2\", patent + \".json.bz2\"\n",
    "                )\n",
    "                if os.path.exists(path):\n",
    "                    data = load_list_bz2(path)\n",
    "                else:\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "        tokens = []\n",
    "        for key, words in data.items():\n",
    "            tokens += [KEY2QUERY[key] + \":\" + word for word in words]\n",
    "        patent2tokens[patent] = set(tokens)\n",
    "\n",
    "    # tokenをカウント\n",
    "    global_token2patents = {}\n",
    "    for patent, tokens in patent2tokens.items():\n",
    "        for token in tokens:\n",
    "            if token not in global_token2patents:\n",
    "                global_token2patents[token] = []\n",
    "            global_token2patents[token].append(patent)\n",
    "    for key in global_token2patents.keys():\n",
    "        global_token2patents[key] = set(global_token2patents[key])\n",
    "\n",
    "    # centerごとにpatentを整理\n",
    "    # center -> List[patent]\n",
    "    center2patents = {}\n",
    "    for patent in patents:\n",
    "        if patent not in pub2id:\n",
    "            continue\n",
    "        patent_id = pub2id[patent]\n",
    "        if patent_id not in patent2centers:\n",
    "            continue\n",
    "        center_ids = patent2centers[patent_id]\n",
    "        centers = [id2pub[center_id] for center_id in center_ids if center_id in id2pub]\n",
    "        for center in centers:\n",
    "            if center not in center2patents:\n",
    "                center2patents[center] = []\n",
    "            center2patents[center].append(patent)\n",
    "    for center in center2patents.keys():\n",
    "        center2patents[center] = set(center2patents[center])\n",
    "\n",
    "    # centerごとに使用可能なtokenを保存\n",
    "    center_token_inner_outers = []\n",
    "    for center, this_patents in center2patents.items():\n",
    "        token2patents = {}\n",
    "        for patent in this_patents:\n",
    "            if patent not in patent2tokens:\n",
    "                continue\n",
    "            for token in patent2tokens[patent]:\n",
    "                if token not in token2patents:\n",
    "                    token2patents[token] = []\n",
    "                token2patents[token].append(patent)\n",
    "        for key in token2patents.keys():\n",
    "            token2patents[key] = set(token2patents[key])\n",
    "\n",
    "        used_set = set()\n",
    "        for token, inner_patents in token2patents.items():\n",
    "            if len(global_token2patents[token]) > 5 + len(inner_patents):\n",
    "                continue\n",
    "\n",
    "            # inner, outerの個数を整理\n",
    "            all_patents = global_token2patents[token]\n",
    "            outer_patents = all_patents - inner_patents\n",
    "            inner_count = len(inner_patents)\n",
    "            outer_count = len(outer_patents)\n",
    "\n",
    "            # outerが多すぎたらNG\n",
    "            if outer_count >= 5:\n",
    "                continue\n",
    "\n",
    "            # innerが少なすぎたらNG\n",
    "            if inner_count <= 1:\n",
    "                continue\n",
    "\n",
    "            # まったく同じpatentならNG\n",
    "            key = tuple(sorted(list(all_patents)))\n",
    "            if key in used_set:\n",
    "                continue\n",
    "            used_set.add(key)\n",
    "\n",
    "            center_token_inner_outers.append(\n",
    "                (center, token, list(inner_patents), list(outer_patents))\n",
    "            )\n",
    "\n",
    "    # if len(center_token_inner_outers):\n",
    "    #     print(center, center_token_inner_outers)\n",
    "\n",
    "    save_list_bz2(center_token_inner_outers, output_path)\n",
    "\n",
    "    # del (\n",
    "    #     global_token2patents,\n",
    "    #     center2patents,\n",
    "    #     patent2tokens,\n",
    "    #     center_token_inner_outers,\n",
    "    # )\n",
    "    # try:\n",
    "    #     del token2patents, used_set, all_patents\n",
    "    #     del outer_patents, inner_patents, inner_count, outer_count, key\n",
    "    # except:\n",
    "    #     pass\n",
    "    # try:\n",
    "    #     del center_ids, centers, this_patents, tokens, data, path\n",
    "    # except:\n",
    "    #     pass\n",
    "    # gc.collect()\n",
    "\n",
    "\n",
    "# for cpc, patents in tqdm(cpc2patents.items(), total=len(cpc2patents)):\n",
    "#     process_cpc_patents(cpc, patents)\n",
    "\n",
    "with Pool(16) as p:\n",
    "    _ = list(\n",
    "        tqdm(\n",
    "            p.imap_unordered(process_cpc_patents, cpc2patents.items()),\n",
    "            total=len(cpc2patents),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
