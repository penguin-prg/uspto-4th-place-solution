{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import whoosh\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from whoosh.analysis import StandardAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "\n",
    "def save_list_bz2(data, filename):\n",
    "    serialized_data = json.dumps(data)\n",
    "    compressed_data = bz2.compress(serialized_data.encode(\"utf-8\"))\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(compressed_data)\n",
    "\n",
    "\n",
    "def load_list_bz2(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "        decompressed_data = bz2.decompress(compressed_data)\n",
    "        return json.loads(decompressed_data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/token-counts/token_counts.json\", \"r\") as f:\n",
    "    token_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 290823/1330766 [09:07<49:13, 352.05it/s]  "
     ]
    }
   ],
   "source": [
    "key2query = {\n",
    "    \"title\": \"ti\",\n",
    "    \"abstract\": \"ab\",\n",
    "    \"claims\": \"clm\",\n",
    "    \"description\": \"detd\",\n",
    "}\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    # 出力ファイル名の生成\n",
    "    output_dir = \"rare_token\"\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path).replace(\".bz2\", \".json\"))\n",
    "\n",
    "    # 既にファイルが存在する場合は処理をスキップ\n",
    "    if os.path.exists(output_file):\n",
    "        return\n",
    "\n",
    "    # データ読み込み\n",
    "    data = load_list_bz2(file_path)\n",
    "\n",
    "    patent2unique_token = {}\n",
    "    for pat, pat_tokens in data.items():\n",
    "        count_token = []\n",
    "        for key, tokens in pat_tokens.items():\n",
    "            tokens = set(tokens)\n",
    "            for token in tokens:\n",
    "                count_token.append((token_counts[key][token], key2query[key] + \":\" + token))\n",
    "        count_token = sorted(count_token)[:4]\n",
    "        patent2unique_token[pat] = count_token\n",
    "\n",
    "    # 安全なファイル書き込み\n",
    "    safe_write(patent2unique_token, output_file)\n",
    "\n",
    "\n",
    "def safe_write(data, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, dir=os.path.dirname(path), mode=\"w\") as tmp_file:\n",
    "        json.dump(data, tmp_file)\n",
    "    os.replace(tmp_file.name, path)\n",
    "\n",
    "\n",
    "files = sorted(glob(\"/kaggle/input/all-index/patent2data/*.bz2\"))\n",
    "\n",
    "with multiprocessing.Pool(processes=30) as pool:\n",
    "    list(tqdm(pool.imap(process_file, files), total=len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9600.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob(\"rare_token/*.json\"))\n",
    "patent2unique_token = {}\n",
    "for file in tqdm(files):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        for pat, token in data.items():\n",
    "            patent2unique_token[pat] = token\n",
    "print(len(patent2unique_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"patent2rare_token.json\", \"w\") as f:\n",
    "    json.dump(patent2unique_token, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
